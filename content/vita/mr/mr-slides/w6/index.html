<!DOCTYPE html>
<html>
  <head>
    <title>Partial and Semi-Partial Correlations</title>
    <meta charset="utf-8">
    <meta name="author" content="Daniel Anderson" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/uo.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/uo-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/hygge.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Partial and Semi-Partial Correlations
### Daniel Anderson
### Week 6

---



# Agenda 
* Review Lab
* A few more notes on model assumptions
* Partial and semi-partial correlations
* Lab

---
class: inverse middle center
background-image:url(../img/chalkboard.jpg)
background-size:cover

# What questions do you have?


---
class: inverse middle center
background-image:url(../img/wood.jpg)
background-size:cover

# Let's get some data

---
# Residuals
Last time, we talked about: 

--
* ordinary residuals: `\(Y' - Y\)`

--
* Standardized residuals: `\(e_{Si} = \frac{e_i}{s_{y.x}\sqrt{1 - h_i}}\)`


---
# Studentized deleted residuals

.Large[

$$
e\_{Ti} = \frac{e\_i}{s\_{y.x\_{(-i)}}\sqrt{1 - h\_i}}
$$

]

* Here `\(s_{y.x_{(-i)}}\)` represents the residual standard error when
  observation `\(i\)` is removed.


--
### My point in showing you this
* You'll likely see these elsewhere
* Some people will use these values or standardized residuals in plots against 
  the fitted values. 
  
---
class: inverse middle

.Large[

&gt; While [standardized residuals] have constant variance, they are no longer uncorrelated with the fitted values or linear combinations of the regressors, so using standardized residuals in plots is not an obvious improvement.

]

\-John Fox

---
# One benefit
* Studentized residuals should follow a `\(T\)` distribution, so they can be
  another used as another method for testing for outliers.

--
* Use the {arm} library (applied regression modeling). Has lots of other good
  functions as well.

---

```r
*library(car) #new library
m1 &lt;- lm(sex_risk ~ internalizing + externalizing + aces, d)
qqPlot(m1)
```

![](w6_files/figure-html/car-t-eval-1.png)&lt;!-- --&gt;

```
## [1] 25 98
```

---
# Actual tests

(also from {car})


```r
outlierTest(m1)
```


```
## No Studentized residuals with Bonferonni p &lt; 0.05
## Largest |rstudent|:
##    rstudent unadjusted p-value Bonferonni p
## 25 3.473468         0.00072715     0.086531
```

--
* Ultimately, this is another source of information, but just because an  
  outlier is "significant", doesn't mean it should be removed. Influence still 
  matters more.

---
# dfbeta

Cook's D measures the influence of a particular point on the entire regression.

--

dfbeta measures the influence of a particular point on each coefficient.

--

.Large[

$$
dfbeta\_{ij} = b\_{(-i)j} - b\_j \text{ for } j = 0, \dots , k
$$

]


---


```r
m1_dfb &lt;- dfbeta(m1)
head(m1_dfb)
```

```
##    (Intercept) internalizing externalizing          aces
## 1 -0.001215989 -8.569162e-04  0.0004335400  0.0001554042
## 2  0.011427640 -2.704476e-04  0.0004272696 -0.0030745544
## 3 -0.004314951  1.429682e-05  0.0001019861  0.0006437915
## 4 -0.009089639  3.853964e-04 -0.0006347214  0.0022869879
## 5  0.009333615  9.984149e-04 -0.0018963805  0.0044205480
## 6 -0.005674869 -1.576898e-04  0.0004064947 -0.0007107409
```

---
# Make it a df

.gray[Note, this may be slightly more complicated if you have missing data]


```r
m1_dfb &lt;- as.data.frame(m1_dfb) %&gt;%
  janitor::clean_names() %&gt;%
  mutate(id = d$id) 
```

---
# Arrange by a coefficient


```r
m1_dfb %&gt;% 
  arrange(desc(intercept)) %&gt;% # Arrange by intercept
  head()
```

```
##    intercept internalizing externalizing         aces id
## 1 0.02867465 -4.262090e-04 -4.685037e-04 -0.003876467 17
## 2 0.02765427 -3.212420e-03  9.407859e-04 -0.005261787 16
## 3 0.02621188  5.973369e-05 -6.396863e-04 -0.004306042 79
## 4 0.02162779 -5.528834e-04 -2.071704e-04 -0.002643075 56
## 5 0.02064415 -6.571181e-04 -6.527562e-05 -0.002518157 92
## 6 0.01727215 -4.415376e-04 -1.654481e-04 -0.002110783 75
```


---
# Arrange by disp


```r
m1_dfb %&gt;%
  arrange(desc(internalizing)) %&gt;% # Arrange by internalizing
  head()
```

```
##       intercept internalizing externalizing          aces  id
## 1  0.0127713066   0.005231534  -0.002936985 -0.0203117601  98
## 2 -0.0082862363   0.004348360  -0.002622956 -0.0085160665 104
## 3 -0.0123404946   0.002142599  -0.001146647 -0.0021512821  83
## 4 -0.0120504854   0.001926893  -0.001206499 -0.0005073047  96
## 5 -0.0023534288   0.001876594  -0.001990900 -0.0005798068 116
## 6  0.0004044204   0.001872995  -0.001701031  0.0015975996 118
```


---
# Big picture
This is just more information that may be useful when diagnosing whether or not
you have met the assumptions of your model, and/or the influence of outliers on
your results.

---
class: middle center
# Partial and semi-partial correlations

![](w6_files/figure-html/plot-1.png)&lt;!-- --&gt;

---
# Zero-order correlation

Correlation between two variables, without accounting for any other variable

--
![](../img/zero-order.png)

---
# Examine zero order correlations


```r
d %&gt;%
  select(sex_risk, internalizing, externalizing, aces) %&gt;%
  cor()
```

```
##                sex_risk internalizing externalizing      aces
## sex_risk      1.0000000     0.3263386     0.4366590 0.3465486
## internalizing 0.3263386     1.0000000     0.7452254 0.3679023
## externalizing 0.4366590     0.7452254     1.0000000 0.2559982
## aces          0.3465486     0.3679023     0.2559982 1.0000000
```

---
# Plot 
Base version, or use {GGally}


```r
d %&gt;%
  select(sex_risk, internalizing, externalizing, aces) %&gt;%
  pairs()
```

---
![](w6_files/figure-html/pairs-plot-eval-1.png)&lt;!-- --&gt;

---
background-image:url(../img/multiple-r.png)
background-size:contain

# Multiple `\(R\)` &amp; `\(R^2\)`

---
class: middle
background-image:url(../img/partial1.png)
background-size:contain

# Partial `\(r\)`

---
class: middle
background-image:url(../img/partial2.png)
background-size:contain

# Partial `\(r\)`


---
# Partial correlations

Represent the *unique* relation between the DV and one IV when common 
variance from other predictors has been removed (partialed, residualized) 
from **both** variables. 

--
* Largely represent what is happening "under the hood"


--
### The problem with partial correlations
* The denominator differs for each partial correlation
  + Not directly comparable
  + `\(a + e \neq b + e\)` 

---
class: left
background-image:url(../img/semi-partial1.png)
background-size:contain

# Semi-partial `\(r\)`

---
class: left
background-image:url(../img/semi-partial2.png)
background-size:contain

# Semi-partial `\(r\)`

---
# Semi-partial

Represent the unique relation between the DV and the IV when common variance
among predictors has been removed (partialed, residualized)

--
* Same denominator for all predictor variables - directly comparable

--
* When squared, represents the **unique variance accounted for by the model**.
This can be powerful.

--
* Squared value is on the same scale as `\(R^2\)`

---
# Discussion

1. Which do you think is more useful for interpretation?
2. Will the sum of the squared semipartial correlations always equal
`\(R^2\)`?

---
class: inverse center middle
# Estimation

---
# Going back to our model

$$
risk_i = b_0 + b_1(internalizing_i) + b_2(externalizing_i) + b_3(aces_i) + e
$$

--
### Fit


```r
m1 &lt;- lm(sex_risk ~ internalizing + externalizing + aces, d)
```

---


```r
summary(m1, detail = TRUE)
```

```
## 
## Call:
## lm(formula = sex_risk ~ internalizing + externalizing + aces, 
##     data = d)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.5070 -0.4682 -0.1592  0.4652  1.9832 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -0.456241   0.106230  -4.295 3.67e-05 ***
## internalizing -0.009337   0.011064  -0.844  0.40048    
## externalizing  0.030890   0.008364   3.693  0.00034 ***
## aces           0.089738   0.028655   3.132  0.00220 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.6181 on 115 degrees of freedom
*## Multiple R-squared:  0.2543,	Adjusted R-squared:  0.2348 
*## F-statistic: 13.07 on 3 and 115 DF,  p-value: 2.098e-07
```

---
# Compute partial and semipartials

{lmSupport}


```r
# install.packages("lmSupport")
library(lmSupport)
modelEffectSizes(m1)
```

```
## lm(formula = sex_risk ~ internalizing + externalizing + aces, 
##     data = d)
## 
## Coefficients
*##                  SSR df pEta-sqr dR-sqr
## (Intercept)   7.0479  1   0.1382     NA
## internalizing 0.2721  1   0.0062 0.0046
## externalizing 5.2115  1   0.1060 0.0884
## aces          3.7473  1   0.0786 0.0636
## 
## Sum of squared errors (SSE): 43.9
## Sum of squared total  (SST): 58.9
```

---
# Model interpretation
* Note the values on the prior slide were squared (this is good, this is
generally what we want to interpret)


--
### Things should interpret for all models
* Coefficients of interest (you can often skip over "control" variables)
* Significance
* Confidence intervals
* Model fit (i.e., `\(R^2\)`)


--
In many cases, squared semi-partial correlations can add to the substantive
interpretation - what is the unique variance accounted for by a specific
predictor variable?


---
# Example 
### `\(R^2\)`
The combination of internalizing and externalizing behaviors, along with ACEs
scores, accounted for approximately 25% of the total variability in sexual risk
scores, which was significant, `\(F(3, 115) = 13.07, p &lt; 0.05\)`. 

---
# Example (coefficients)
### Intercept
The model indicated that girls who exhibited no internalizing or externalizing
behaviors, and scored a zero on ACEs, would be expected to score -0.43 points 
on the sexual risk composite variable, on average. 

---
# Example (coefficients)
### Internalizing behaviors

The internalizing behaviors measure was not a significant predictor of girls
sexual risk score, `\(t(115) = -0.84, p = 0.40\)`, when controlling for
externalizing behaviors and
ACEs scores. 

---
# Example (coefficients)
### Externalizing behaviors

The externalizing measure was significant, `\(t(115) = 3.69, p &lt; 0.05\)`,
indicating that a one unit increase in externalizing behaviors corresponded to,
on average, a 0.03 unit increase on the sexual risk composite, with a 95%
confidence interval indicating the true effect may lie between 0.01 and 0.05. 
Externalizing behavior uniquely accounted for 8.8% of the total variability in
sexual risk scores, when controlling for internalizing behaviors and ACES
scores. 


---
# Example (coefficients)
### ACEs

Finally, the ACEs score was also significant, `\(t(115) = 3.13, p &lt; 0.05\)`, 
indicating that a one unit increase on ACES corresponded to, on average, a
0.09 unit increase in sexual risk behavior, `\(95% CI = [0.03, 0.15]\)`. The ACEs
measure uniquely accounted for 6.4% of the total variability in sexual risk
scores, after accounting for internalizing and externalizing behaviors.


---
# What about tables?
* Generally, if you fit a regression model, you'll want to have a table
reporting all of this stuff.


--
### My view
* If your table is detailed enough, don't report all the stats again in text 
(keep the text mostly on the substantive side)


---
class: right
background-image:url(../img/mr-table.png)
background-size:contain

# Example table

---
# Revised interpretation

Table 3 reports the parameter estimates from the model, which accounted for 25%
of the total variability in sexual risk behavior scores. Both the externalizing
behavior and the ACEs measures significantly predicted sexual risk behavior
scores, while internalizing behaviors was not significant. 
Externalizing behavior scores uniquely accounted for 8.8% of the total
variability in sexual risk scores, while ACEs uniquely accounted for 6.4% of 
the total variability, after accounting for the other variables in the model. 
As can be seen, the confidence intervals indicated that, while
significant, there was considerable uncertainty in the ACEs estimate, with 95%
confidence interval ranging from 0.03 to 0.15. 

---
# Figures
* Often it's helpful to include figures, specifically if you can show what your
model is doing.
* This particular version is a little complicated

--
* First, think about what you want to show. How about externalizing behavior
predicting sexual risk, holding internalizing behaviors constant, and for a
couple of different levels of ACES (i.e., separate lines for each ACES level we
choose)?

---
### Create some data


```r
x_axis &lt;- seq(from = 0, to = 45, by = .1)

pred_frame &lt;- data.frame(internalizing = 0, 
                         externalizing = rep(x_axis, 3),
                         aces = rep(c(0, 3, 6), 
                                    each = length(x_axis))) 
  
pred_frame &lt;- pred_frame %&gt;%
  mutate(pred = predict(m1, newdata = pred_frame),
         aces = as.factor(aces))  # Why change to factor?
```

---

```r
head(pred_frame)
```

```
##   internalizing externalizing aces       pred
## 1             0           0.0    0 -0.4562407
## 2             0           0.1    0 -0.4531517
## 3             0           0.2    0 -0.4500627
## 4             0           0.3    0 -0.4469736
## 5             0           0.4    0 -0.4438846
## 6             0           0.5    0 -0.4407956
```

```r
tail(pred_frame)
```

```
##      internalizing externalizing aces     pred
## 1348             0          44.5    6 1.456808
## 1349             0          44.6    6 1.459897
## 1350             0          44.7    6 1.462986
## 1351             0          44.8    6 1.466075
## 1352             0          44.9    6 1.469164
## 1353             0          45.0    6 1.472253
```

---
# Create plot
First, let's just plot externalizing behavior again sexual risk (something we
would likely have already done before fitting the model)


```r
ggplot(d, aes(externalizing, sex_risk)) +
  geom_point() 
```

![](w6_files/figure-html/pred-plot1-1.png)&lt;!-- --&gt;

---
# Add lines from the model

Notice we change the data source when we add lines, and we change the y-axis.


```r
ggplot(d, aes(externalizing, sex_risk)) +
  geom_point() +
  geom_line(aes(y = pred, color = aces), 
            data = pred_frame)
```

![](w6_files/figure-html/pred-plot2-1.png)&lt;!-- --&gt;


---
# Also useful - coefficient plots
* You may have seen these in DataCamp
* Requires the {broom} library


```r
library(broom)
tidy_pd &lt;-  tidy(m1, conf.int = TRUE)
tidy_pd  
```

```
## # A tibble: 4 x 7
##   term            estimate   std.error  statistic      p.value    conf.low
##   &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;
## 1 (Intercept) -0.4562407   0.1062302   -4.294833   3.671069e-5 -0.6666622 
## 2 internaliz… -0.009337001 0.01106415  -0.8438968  4.004791e-1 -0.03125295
## 3 externaliz…  0.03089038  0.008364275  3.693133   3.404187e-4  0.01432236
## 4 aces         0.08973776  0.02865517   3.131643   2.204678e-3  0.03297739
## # ... with 1 more variable: conf.high &lt;dbl&gt;
```

---
# Plot coefficients w/CIs


```r
ggplot(tidy_pd, aes(term, estimate)) +
  geom_hline(yintercept = 0, 
             color = "cornflowerblue",
             size = 2) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymin = conf.low, 
                    ymax = conf.high)) +
  coord_flip()
```

![](w6_files/figure-html/coefplot-1.png)&lt;!-- --&gt;

---
class: inverse middle

# Revisiting Estimation
Let's calculate the partial correlation between externalizing behavior and
sexual risk.

---
# Step 1
Residualize the outcome with respect to the other predictors


```r
y_noex &lt;- lm(sex_risk ~ internalizing + aces, d)
```

--
### Why?

---
background-image:url(../img/partial3.png)
background-size:contain


--
# Step 2
Extract residualized values


```r
y_res &lt;- residuals(y_noex)
```

---
# Step 3
Residualize the predictor with respect to the other predictors


```r
ex_mod &lt;- lm(externalizing ~ internalizing + aces, d)
```

--
### Why?

---
background-image:url(../img/partial3.png)
background-size:contain


--
# Step 4
Extract residualized values


```r
ex_res &lt;- residuals(ex_mod)
```

---
# Compute correlation


```r
cor(y_res, ex_res)
```

```
## [1] 0.3256179
```

--
### semi-partial correlation


```r
cor(d$sex_risk, ex_res)
```

```
## [1] 0.2973974
```


--
* The only difference between the partial and semi-partial correlation
is whether the residualized predictor is correlated with the raw data for the
outcome, or the residualized outcome.

--
* Semi-partial correlations are more interpretable, but will generally always 
be smaller than the partial correlations (think about the denominator)


---
# Square the values
* Squaring the semipartial correlation gives us the variance uniquely accounted
for by the variable.
* Squaring the partial correlation gives us partial `\(\eta^2\)`, which is not
terrifically interpretable.

--


```r
cor(y_res, ex_res)^2
```

```
## [1] 0.106027
```

```r
cor(d$sex_risk, ex_res)^2
```

```
## [1] 0.08844522
```

---

```r
modelEffectSizes(m1)
```

```
## lm(formula = sex_risk ~ internalizing + externalizing + aces, 
##     data = d)
## 
## Coefficients
##                  SSR df pEta-sqr dR-sqr
## (Intercept)   7.0479  1   0.1382     NA
## internalizing 0.2721  1   0.0062 0.0046
## externalizing 5.2115  1   0.1060 0.0884
## aces          3.7473  1   0.0786 0.0636
## 
## Sum of squared errors (SSE): 43.9
## Sum of squared total  (SST): 58.9
```

---
## Alternative squared semi-partial `\(r\)` calculation
A more intuitive way to think about squared semi-partial correlations is by
thinking about two separate models, one with and without the variable of
interest.

--
* What's the difference in `\(R^2\)`? That's your squared semi-partial correlation.
* Let's calculate the squared semi-partial correlation for externalizing
behaviors again using this method

--

```r
reduced_model &lt;- lm(sex_risk ~ internalizing + aces, d)
full_model &lt;- lm(sex_risk ~ internalizing + externalizing + aces, d)
```

---


```r
arm::display(full_model)
```

```
## lm(formula = sex_risk ~ internalizing + externalizing + aces, 
##     data = d)
##               coef.est coef.se
## (Intercept)   -0.46     0.11  
## internalizing -0.01     0.01  
## externalizing  0.03     0.01  
## aces           0.09     0.03  
## ---
## n = 119, k = 4
## residual sd = 0.62, R-Squared = 0.25
```

```r
arm::display(reduced_model)
```

```
## lm(formula = sex_risk ~ internalizing + aces, data = d)
##               coef.est coef.se
## (Intercept)   -0.44     0.11  
## internalizing  0.02     0.01  
## aces           0.09     0.03  
## ---
## n = 119, k = 3
## residual sd = 0.65, R-Squared = 0.17
```


---
# `\(\Delta R^2\)`

.Large[
$$
0.25 - 0.17 = 0.08
$$

]

* When you're comparing models and the only difference is **one** variable 
being included versus not included, `\(\Delta R^2\)` will equal the squared
semi-partial correlation
* Take the square root if you (for some reason) want the semi-partial
correlation.
* Use `summary` instead of `arm::display` for more decimal places.

---
# Take home message
* It's important that you have an intuitive understanding of semi-partial
correlations in particular (partial correlations less important)
* `\(\Delta R^2\)` method probably most intuitive


--
### Moving forward
* We have an entire week devoted to model comparisons. We'll talk about 
`\(\Delta R^2\)` more then

---
class: inverse center middle
# Lab
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "atelier-dune-light",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
